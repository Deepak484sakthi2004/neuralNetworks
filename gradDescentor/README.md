# gradDescentor - Neural Network from Scratch

Welcome to gradDescentor, a minimalist neural network implementation from scratch. This project introduces a compact Autograd engine and a concise neural networks library, showcasing the essentials of building and training neural networks without relying on external deep learning libraries.

## Features

- **Neural Networks Library:** A compact library (50 lines) offering a PyTorch-like API for constructing and training neural networks.
- **Custom Implementation:** Build neural networks from the ground up, gaining a deep understanding of the core concepts.
- **Custom Implementation:** Build neural networks from scratch without external deep learning libraries.
- **Backpropagation:** Illustrates the backpropagation algorithm for training the neural network.
- **Gradient Descent:** Utilizes gradient descent to optimize weights during training.
- **Predictions:** Once trained, the model can make predictions on new data through forward-passing.

## Getting Started

# Example Usage
```python
a = Value(-12.0)
b = Value(8.0)
c = a + b
g += 9.0 / f
print(f'{g.data:.4f}')  # prints the outcome of this forward pass
g.backward()
print(f'{a.grad:.4f}')  # prints the numerical value of dg/da
print(f'{b.grad:.4f}')  # prints the numerical value of dg/db
```

